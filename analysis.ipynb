{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Pipeline from the paper [Persistence diagrams as morphological signatures of cells: A method to measure and compare cells within a population](https://files.yossi.eu/manuscripts/2310.20644.pdf) by Yossi Bokor Bleile, Pooja Yadav, Patrice Koehl, and Florian Rehfeldt.\n",
    "\n",
    "The following packages need to be installed: \n",
    "- [Correa](correa.yossi.eu)\n",
    "- plotly\n",
    "- pandas\n",
    "- sklearn\n",
    "- numpy\n",
    "- matplotlib\n",
    "- tifffile\n",
    "\n",
    "\n",
    "We begin the analysis by obtaining a persistence diagram for each cell in the population, as a summary of the morophology of the cell.\n",
    "\n",
    "Below is the anlysis for `X1`, which can be repeated for `Y1` by replacing `X1` with `Y1` as appropriate and the running the relevant cells again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import correa\n",
    "import pandas\n",
    "import tifffile\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\t\t\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from sklearn import manifold, cluster, decomposition, metrics, preprocessing\n",
    "import numpy\n",
    "import scipy.cluster\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import ClusterWarning\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.spatial.distance import squareform\n",
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\", ClusterWarning)\n",
    "\n",
    "pio.renderers.default = \"browser\" #set the renderer to browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions for the analysis\n",
    "\n",
    "We next define some custom functions to make creating the dendrograms easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set colours\n",
    "def rgb_to_hex(r, g, b):\n",
    "    return '#{:02x}{:02x}{:02x}'.format(r, g, b)\n",
    "hex_list = []\n",
    "for c in px.colors.qualitative.Set1:\n",
    "\thex_list.append(rgb_to_hex(int(c.replace(\"rgb(\",\"\").replace(\")\",\"\").split(\",\")[0]),int(c.replace(\"rgb(\",\"\").replace(\")\",\"\").split(\",\")[1]), int(c.replace(\"rgb(\",\"\").replace(\")\",\"\").split(\",\")[2])))\n",
    " \n",
    " \n",
    "def plot_dendrogram(model, **kwargs):\n",
    "\t# Create linkage matrix and then plot the dendrogram\n",
    "\t# create the counts of samples under each node\n",
    "\tcounts = numpy.zeros(model.children_.shape[0])\n",
    "\tn_samples = len(model.labels_)\n",
    "\tfor i, merge in enumerate(model.children_):\n",
    "\t\tcurrent_count = 0\n",
    "\t\tfor child_idx in merge:\n",
    "\t\t\tif child_idx < n_samples:\n",
    "\t\t\t\tcurrent_count += 1  # leaf node\n",
    "\t\t\telse:\n",
    "\t\t\t\tcurrent_count += counts[child_idx - n_samples]\n",
    "\t\tcounts[i] = current_count\n",
    "\tlinkage_matrix = numpy.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
    "\t# Plot the corresponding dendrogram\n",
    "\tplt.figure(figsize=(2000,2000))\n",
    "\tplt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "\tplt.tick_params(\n",
    "    axis='y',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "\tdendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "def generate_rand_index(cluster_df : pandas.DataFrame, to_compare : list):\n",
    "\tr_ind = pandas.DataFrame(columns=to_compare, index=to_compare)\n",
    "\tfor i in to_compare:\n",
    "\t\tfor j in to_compare:\n",
    "\t\t\tr_ind[i].loc[j] = metrics.rand_score(cluster_df[i], cluster_df[j])\n",
    "\treturn r_ind\n",
    "\n",
    "def population_percentages(df : pandas.DataFrame, clustering : str):\n",
    "\tlabels = [int(c) for c in df[clustering]]\n",
    "\tcounts = [0 for i in range(max(labels)+1)]\n",
    "\tfor l in labels:\n",
    "\t\tcounts[l]+=1\n",
    "\tpercentages = [c/len(labels) for c in counts]\n",
    "\treturn percentages, counts\n",
    "\n",
    "def analysis(dists : pandas.DataFrame, cluster_numbers : list,  name : str,  dir : str, group = False, exclude : list = [], show2d = False, showElbow=False, colour_list=hex_list):\n",
    "\t#colour order is red, purple, blue, green\n",
    "\tif group == False:\n",
    "\t\tinds = dists.index\n",
    "\t\tdists = dists\n",
    "\t\tif len(exclude) != 0:\n",
    "\t\t\tinds = []\n",
    "\t\t\tfor f in dists.index:\n",
    "\t\t\t\tif f not in exclude:\n",
    "\t\t\t\t\tinds.append(f)\n",
    "\t\t\tdists = dists[inds].loc[inds]\n",
    "\telse:\n",
    "\t\tinds = []\n",
    "\t\tfor f in dists.index:\n",
    "\t\t\tif group in f and f not in exclude:\n",
    "\t\t\t\tinds.append(f)\n",
    "\t\tdists = dists[inds].loc[inds]\n",
    "\tdf = pandas.DataFrame(index=inds)\n",
    "\tembed = manifold.MDS(3, dissimilarity='precomputed', random_state=1, normalized_stress=\"auto\").fit_transform(dists.to_numpy())\n",
    "\tdf[\"x\"] = embed[:,0]\n",
    "\tdf[\"y\"] = embed[:,1]\n",
    "\tdf[\"z\"] = embed[:,2]\n",
    "\tsilhouette_samples = []\n",
    "\tsilhouette_score = []\n",
    "\tlinkage_matrices = []\n",
    "\tavg = cluster.AgglomerativeClustering(distance_threshold=None, n_clusters=4, linkage=\"average\")\n",
    "\tavg = avg.fit(dists.to_numpy())\n",
    "\tdflt = \"#000000\"\n",
    "\tD_leaf_colors = {dists.index[i]: colour_list[avg.labels_[i]] for i in range(len(avg.labels_))}\n",
    "\t#Average\n",
    "\tfor link in [\"average\", \"complete\", \"single\", \"ward\"]:\n",
    "\t\tlinkage = cluster.AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage=link)\n",
    "\t\tlinkage = linkage.fit(dists.to_numpy())\n",
    "\t\tcounts = numpy.zeros(linkage.children_.shape[0])\n",
    "\t\tn_samples = len(linkage.labels_)\n",
    "\t\tfor i, merge in enumerate(linkage.children_):\n",
    "\t\t\tcurrent_count = 0\n",
    "\t\t\tfor child_idx in merge:\n",
    "\t\t\t\tif child_idx < n_samples:\n",
    "\t\t\t\t\tcurrent_count += 1  # leaf node\n",
    "\t\t\t\telse:\t\n",
    "\t\t\t\t\tcurrent_count += counts[child_idx - n_samples]\n",
    "\t\t\tcounts[i] = current_count\n",
    "\t\tlinkage_matrix = numpy.column_stack([linkage.children_, linkage.distances_, counts]).astype(float)\n",
    "\t\tlinkage_matrices.append(linkage_matrix)\n",
    "\t\t# notes:\n",
    "\t\t# * rows in Z correspond to \"inverted U\" links that connect clusters\n",
    "\t\t# * rows are ordered by increasing distance\n",
    "\t\t# * if the colors of the connected clusters match, use that color for link\n",
    "\t\tlink_cols = {}\n",
    "\t\tfor i, i12 in enumerate(linkage_matrix[:,:2].astype(int)):\n",
    "\t\t\tc1, c2 = (link_cols[x] if x > len(linkage_matrix) else D_leaf_colors[dists.index[x]] for x in i12)\n",
    "\t\t\tif c1 == c2:\n",
    "\t\t\t\tlink_cols[i+1+len(linkage_matrix)] = c1\n",
    "\t\t\telse:\n",
    "\t\t\t\tif i12[0] < n_samples:\n",
    "\t\t\t\t\tlink_cols[i+1+len(linkage_matrix)] = D_leaf_colors[dists.index[i12[0]]]\n",
    "\t\t\t\telif i12[1] < n_samples:\n",
    "\t\t\t\t\tlink_cols[i+1+len(linkage_matrix)] = D_leaf_colors[dists.index[i12[1]]]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tlink_cols[i+1+len(linkage_matrix)] = dflt\n",
    "\t\t# Dendrogram\n",
    "\t\tD = dendrogram(Z=linkage_matrix, labels=dists.index, color_threshold=None, no_labels=True, link_color_func=lambda x: link_cols[x])\n",
    "\t\t#plt.xlabel(name+\" \"+link)\n",
    "\t\tplt.yticks([])\n",
    "\t\tplt.savefig(dir+\"/\"+name.replace(\" \",\"_\")+\"_\"+link+\"_dendrogram.png\")\n",
    "\t\tplt.show()\n",
    "\tsse = []\n",
    "\tfor link in [\"average\", \"complete\", \"single\", \"ward\"]:\n",
    "\t\tfor k in cluster_numbers:\n",
    "\t\t\tlinkage = cluster.AgglomerativeClustering(distance_threshold=None, n_clusters=k, linkage=link)\n",
    "\t\t\tlinkage = linkage.fit(dists.to_numpy())\n",
    "\t\t\tlabels = linkage.labels_\n",
    "\t\t\tlabels = [str(c) for c in labels]\n",
    "\t\t\tdf[link+str(k)] = labels\n",
    "\t\t\tsil_score = metrics.silhouette_score(dists, df[link+str(k)], metric=\"precomputed\")\n",
    "\t\t\tsilhouette_score.append(sil_score)\n",
    "\t\t\tsil_samps = metrics.silhouette_samples(dists, df[link+str(k)], metric=\"precomputed\")\n",
    "\t\t\tsilhouette_samples.append([sil_samps])\n",
    "\t\t\tfig = px.scatter(df, x='x', y='y',color=link+str(k), title=name+\" (\"+link+\" \"+str(k)+\")\", hover_data=[df.index], width=800, height=600, color_discrete_map={\n",
    "                \"0\": hex_list[0], \"1\": hex_list[1],\"2\": hex_list[1],\"2\": hex_list[2], \"3\": hex_list[3]})\n",
    "\t\t\tfig.update_traces(marker={'size': 5})\n",
    "\t\t\tif show2d:\n",
    "\t\t\t\tfig.show()\n",
    "\t\t\tfig.write_image(dir+\"/\"+name.replace(\" \",\"_\")+\"_\"+link+str(k)+\"_2D.png\")\n",
    "\tif showElbow:\n",
    "\t\tplt.plot(cluster_numbers, sse)\n",
    "\t\tplt.title(\"Elbow Method\")\n",
    "\t\tplt.xlabel(\"Number of Clusters\")\n",
    "\t\tplt.xticks(cluster_numbers)\n",
    "\t\tplt.ylabel(\"SSE\")\n",
    "\t\tplt.savefig(dir+\"/\"+name.replace(\" \",\"_\")+\"_kmeans-elbow.png\")\n",
    "\t\tfig.show()\n",
    "\tdf.to_csv(dir+\"/\"+name.replace(\" \",\"_\")+\"_df.csv\")\n",
    "\trand_ind_tables_latex = []\n",
    "\tpercentages_dict = {}\n",
    "\tfor k in cluster_numbers:\n",
    "\t\trand_ind_tables_latex.append(generate_rand_index(df, [\"average\"+str(k), \"complete\"+str(k), \"single\"+str(k),\"ward\"+str(k)]).to_latex())\n",
    "\t\tprint(\"cluster sizes and percentages are:\")\n",
    "\t\tpercentages_dict[\"average{}\".format(k)], counts = population_percentages(df, \"average\"+str(k))\n",
    "\t\tprint(\"average{}\".format(k), counts, percentages_dict[\"average{}\".format(k)])\n",
    "\t\tpercentages_dict[\"complete{}\".format(k)], counts = population_percentages(df, \"complete\"+str(k))\n",
    "\t\tprint(\"complete{}\".format(k), counts, percentages_dict[\"complete{}\".format(k)])\n",
    "\t\tpercentages_dict[\"single{}\".format(k)], counts = population_percentages(df, \"single\"+str(k))\n",
    "\t\tprint(\"single{}\".format(k), counts, percentages_dict[\"single{}\".format(k)])\n",
    "\t\tpercentages_dict[\"ward{}\".format(k)], counts = population_percentages(df, \"ward\"+str(k))\n",
    "\t\tprint(\"ward{}\".format(k), counts, percentages_dict[\"ward{}\".format(k)])\n",
    "\t\t#percentages_dict[\"kmeans++{}\".format(k)] = population_percentages(df, \"kmeans++\"+str(k))\n",
    "\t\t#print(\"kmeans++{}\".format(k), percentages_dict[\"kmeans++{}\".format(k)])\n",
    "\treturn df, silhouette_score, silhouette_samples, rand_ind_tables_latex, percentages_dict, dists, linkage_matrices\n",
    "\n",
    "def get_main_population(analysis, cluster : str, dists : pandas.DataFrame):\n",
    "\tmain_id = analysis[4][cluster].index(max(analysis[4][cluster]))\n",
    "\tmain_index= analysis[0].index[analysis[0][cluster] == str(main_id)]\n",
    "\treturn dists[main_index].loc[main_index]\n",
    "\n",
    "def purity(merges, clus : set, n_objs):\n",
    "\tdct = dict([(i, {i}) for i in range(n_objs)])\n",
    "\tfor i, row in enumerate(merges, n_objs):\n",
    "\t\tdct[i] = dct[row[0]].union(dct[row[1]])\n",
    "\t\tdel dct[row[0]]\n",
    "\t\tdel dct[row[1]]\n",
    "\t\tfor c in list(dct.values()):\n",
    "\t\t\tif clus.issubset(c):\n",
    "\t\t\t\treturn c, (n_objs-len(c))/(n_objs-len(clus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"X1\" #select the dataset to analyse and the set the correct width and height of the images\n",
    "if dataset == \"X1\":\n",
    "\twidth=1500 #we need to set the largest width of an image in the dataset, as FilamentSensor2 pads all the iamges to be the same size\n",
    "\theight=1692 #we need to set the largest height of an image in the dataset, as FilamentSensor2 pads all the iamges to be the same size\n",
    "elif dataset == \"X2\":\n",
    "\twidth=1455\n",
    "\theight=1584\n",
    "elif dataset == \"X3\":\n",
    "\twidth=1919\n",
    "\theight=1467\n",
    "elif dataset == \"Y1\":\n",
    "\twidth=1226 #we need to set the largest width of an image in the dataset, as FilamentSensor2 pads all the iamges to be the same size\n",
    "\theight=1088 #we need to set the largest height of an image in the dataset, as FilamentSensor2 pads all the iamges to be the same size\n",
    "else:\n",
    "\traise ValueError(\"Dataset not supported\")\n",
    "\n",
    "files = os.listdir(dataset+\"/cell/raw_images\") #get all of the files in the cell/raw_images directory\n",
    "cell_names = [int(file[0:3]) for file in files if file.endswith('.tif')] #get the cell numbers from the file names\n",
    "n_cells = len(cell_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the file containing the center of the nucleus for each cell.\n",
    "\n",
    "Next, for each cell will caculate the persistence diagram using a radial function based at the center of the nucles. We do need to translate the center of the nucleus into the same frame as the contour is in. This is due to the way FilamentSensor extracts the contour of each cell in a directory. The appropriate `height` and `width` values can be found in the [FilamentSensor2](https://filament-sensor.de/) log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = pandas.read_csv(dataset+\"/Nuc_Cm_\"+dataset+\".csv\", index_col=\"filename\") #Load the center of the nucleus, labled by the nucleus file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours = [] #list of contours\n",
    "for i in cell_names[0:5]:\n",
    "\tactin = tifffile.imread(dataset+\"/cell/raw_images/\"+str(\"%03d\" % i)+\".tif\") #we need to know the size of the original image to shift the center of the nucleus to the correct position\n",
    "\theight_diff = height - actin.shape[0]\n",
    "\twidth_diff = width - actin.shape[1]\n",
    "\tprint(\"before load\")\n",
    "\tc_i = correa.create_polygon_focal_point(dataset+\"/cell/contours/\"+str(\"%03d\" % i)+\"_contour.csv\", [centers.loc[i,\"X_m\"]+width_diff/2,centers.loc[i,\"Y_m\"]+height_diff/2])\n",
    "\tprint(\"after load\")\n",
    "\tc_i.persistence_diagram()\n",
    "\tprint(\"after persistence diagram\")\n",
    "\tcontours.append(c_i)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the contours and mark the center of the nucleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cell_names[0:5]: #cell number we want to look at\n",
    "\tcontour = pandas.read_csv(dataset+\"/cell/contours/\"+str(\"%03d\" % i)+\"_contour.csv\",header=None)\n",
    "\tactin = tifffile.imread(dataset+\"/cell/raw_images/\"+str(\"%03d\" % i)+\".tif\")\n",
    "\tnucleus = tifffile.imread(dataset+\"/nucleus/raw_images/\"+str(\"%03d\" % i)+\".tif\")\n",
    "\theight_diff = height - actin.shape[0]\n",
    "\twidth_diff = width - actin.shape[1]\n",
    "\tcenter = pandas.DataFrame([[centers.loc[i,\"X_m\"]+width_diff/2,centers.loc[i,\"Y_m\"]+height_diff/2]], columns=[\"x\", \"y\"])\n",
    "\tfig_data = px.scatter(contour, x=0, y=1, width=800, height=600).data\n",
    "\tfig_data = fig_data + px.scatter(center, x=\"x\", y=\"y\").update_traces(marker={'size': 5, 'color': 'Red'}).data\n",
    "\tfig = go.Figure(fig_data)\n",
    "\tfig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a persistence diagram for each cell summarising its morphology, we compute the Wasserstein distance between each pair of persistence diagrams as a (dis)similarity score for each pair of cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_distances = numpy.zeros((n_cells,n_cells))\n",
    "for i in range(n_cells):\n",
    "\tfor j in range(i,n_cells):\n",
    "\t\tdist_ij = correa.wasserstein_distance(contours[i], contours[j], q=2)\n",
    "\t\tdist_ji = correa.wasserstein_distance(contours[j], contours[i], q=2)\n",
    "\t\tdist = (dist_ij+dist_ji)/2\n",
    "\t\tprint(\"for {} and {} dist_ij is {} and dist_ji is {} so we have dist {}\".format(i,j,dist_ij,dist_ji,dist))\n",
    "\t\tw_distances[i,j] = dist\n",
    "\t\tw_distances[j,i] = dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we display a heatmap of the Wasserstein distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(w_distances,width=500, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = pandas.DataFrame(w_distances, columns=cell_names, index=cell_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = analysis(X1_dists, [3,4,5], dataset, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `X1`, the heatmap and all 4 dendrograms indicate there is an outlier, so lets identify which cell this is. Using `average4` the outlier has cluster number 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in A[0].index:\n",
    "\tif int(A[0].loc[i][\"average4\"]) == 3:\n",
    "\t\tprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If desired we can use the `analysis` command with the `exclude` parameter to exclude cell a from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"X1\":\n",
    "\tA_main = analysis(dists, [3,4,5], dataset, dataset, exclude=[\"015\"])\n",
    "\n",
    "\tn_c = A_main[0].shape[0]\n",
    "\tprint(n_c)\n",
    "\tfor k in range(4):\n",
    "\t\tclus = set(())\n",
    "\t\tfor i in range(len(X1_main[0][\"average4\"])):\n",
    "\t\t\tif X1_main[0][\"average4\"].iloc[i] == str(k):\n",
    "\t\t\t\tclus.add(i)\n",
    "\t\tprint(\"cluster \"+str(k))\n",
    "\t\tprint(\"average: \"+str(purity(X1_main[6][0], clus, n_c)[1]))\n",
    "\t\tprint(\"complete: \"+str(purity(X1_main[6][1], clus, n_c)[1]))\n",
    "\t\tprint(\"single: \"+str(purity(X1_main[6][2], clus, n_c)[1]))\n",
    "\t\tprint(\"ward: \"+str(purity(X1_main[6][3], clus, n_c)[1]))\n",
    "\n",
    "else:\n",
    "\traise ValueError(\"Dataset not supported\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
