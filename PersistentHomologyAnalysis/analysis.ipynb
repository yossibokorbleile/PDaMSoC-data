{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistent Homology Analysis of Cell Morphology\n",
    "\n",
    "Pipeline from the paper [Persistence diagrams as morphological signatures of cells: A method to measure and compare cells within a population](https://files.yossi.eu/manuscripts/2310.20644.pdf) by Yossi Bokor Bleile, Pooja Yadav, Patrice Koehl, and Florian Rehfeldt.\n",
    "\n",
    "The following packages need to be installed: \n",
    "- [Correa](https://correa.yossi.eu)\n",
    "- plotly\n",
    "- pandas\n",
    "- sklearn\n",
    "- numpy\n",
    "- matplotlib\n",
    "- tifffile\n",
    "\n",
    "## Overview\n",
    "\n",
    "We begin the analysis by obtaining a persistence diagram for each cell in the population, as a summary of the morphology of the cell. This notebook contains three main analysis sections:\n",
    "\n",
    "1. **X1 Dataset**: Analysis of hMSC cells\n",
    "2. **Y1 Dataset**: Analysis of HeLa cells  \n",
    "3. **X1Y1 Combined Dataset**: Comparative analysis of both cell types together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import correa\n",
    "import pandas\n",
    "import tifffile\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\t\t\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from sklearn import manifold, cluster, decomposition, metrics, preprocessing\n",
    "import numpy\n",
    "import scipy.cluster\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import ClusterWarning\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.spatial.distance import squareform\n",
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\", ClusterWarning)\n",
    "\n",
    "pio.renderers.default = \"browser\" #set the renderer to browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions for the analysis\n",
    "\n",
    "We next define some custom functions to make creating the dendrograms easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set colours\n",
    "def rgb_to_hex(r, g, b):\n",
    "    return '#{:02x}{:02x}{:02x}'.format(r, g, b)\n",
    "hex_list = []\n",
    "for c in px.colors.qualitative.Set1:\n",
    "\thex_list.append(rgb_to_hex(int(c.replace(\"rgb(\",\"\").replace(\")\",\"\").split(\",\")[0]),int(c.replace(\"rgb(\",\"\").replace(\")\",\"\").split(\",\")[1]), int(c.replace(\"rgb(\",\"\").replace(\")\",\"\").split(\",\")[2])))\n",
    " \n",
    " \n",
    "# def plot_dendrogram(model, **kwargs):\n",
    "# \t# Create linkage matrix and then plot the dendrogram\n",
    "# \t# create the counts of samples under each node\n",
    "# \tcounts = numpy.zeros(model.children_.shape[0])\n",
    "# \tn_samples = len(model.labels_)\n",
    "# \tfor i, merge in enumerate(model.children_):\n",
    "# \t\tcurrent_count = 0\n",
    "# \t\tfor child_idx in merge:\n",
    "# \t\t\tif child_idx < n_samples:\n",
    "# \t\t\t\tcurrent_count += 1  # leaf node\n",
    "# \t\t\telse:\n",
    "# \t\t\t\tcurrent_count += counts[child_idx - n_samples]\n",
    "# \t\tcounts[i] = current_count\n",
    "# \tlinkage_matrix = numpy.column_stack([model.children_, model.distances_, counts]).astype(float)\n",
    "# \t# Plot the corresponding dendrogram\n",
    "# \tplt.figure(figsize=(2000,2000))\n",
    "# \tplt.tick_params(\n",
    "#     axis='x',          # changes apply to the x-axis\n",
    "#     which='both',      # both major and minor ticks are affected\n",
    "#     bottom=False,      # ticks along the bottom edge are off\n",
    "#     top=False,         # ticks along the top edge are off\n",
    "#     labelbottom=False) # labels along the bottom edge are off\n",
    "# \tplt.tick_params(\n",
    "#     axis='y',          # changes apply to the x-axis\n",
    "#     which='both',      # both major and minor ticks are affected\n",
    "#     bottom=False,      # ticks along the bottom edge are off\n",
    "#     top=False,         # ticks along the top edge are off\n",
    "#     labelbottom=False) # labels along the bottom edge are off\n",
    "# \tdendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "def generate_rand_index(cluster_df : pandas.DataFrame, to_compare : list):\n",
    "\tr_ind = pandas.DataFrame(columns=to_compare, index=to_compare)\n",
    "\tfor i in to_compare:\n",
    "\t\tfor j in to_compare:\n",
    "\t\t\tr_ind[i].loc[j] = metrics.rand_score(cluster_df[i], cluster_df[j])\n",
    "\treturn r_ind\n",
    "\n",
    "def population_percentages(df : pandas.DataFrame, clustering : str):\n",
    "\tlabels = [int(c) for c in df[clustering]]\n",
    "\tcounts = [0 for i in range(max(labels)+1)]\n",
    "\tfor l in labels:\n",
    "\t\tcounts[l]+=1\n",
    "\tpercentages = [c/len(labels) for c in counts]\n",
    "\treturn percentages, counts\n",
    "\n",
    "def analysis(dists : pandas.DataFrame, cluster_numbers : list,  name : str,  dir : str, group = False, exclude : list = [], show2d = False, showElbow=False, colour_list=hex_list):\n",
    "\t#colour order is red, purple, blue, green\n",
    "\tif group == False:\n",
    "\t\tinds = dists.index\n",
    "\t\tdists = dists\n",
    "\t\tif len(exclude) != 0:\n",
    "\t\t\tinds = []\n",
    "\t\t\tfor f in dists.index:\n",
    "\t\t\t\tif f not in exclude:\n",
    "\t\t\t\t\tinds.append(f)\n",
    "\t\t\tdists = dists[inds].loc[inds]\n",
    "\telse:\n",
    "\t\tinds = []\n",
    "\t\tfor f in dists.index:\n",
    "\t\t\tif group in f and f not in exclude:\n",
    "\t\t\t\tinds.append(f)\n",
    "\t\tdists = dists[inds].loc[inds]\n",
    "\tdf = pandas.DataFrame(index=inds)\n",
    "\tembed = manifold.MDS(3, dissimilarity='precomputed', random_state=1, normalized_stress=\"auto\").fit_transform(dists.to_numpy())\n",
    "\tdf[\"x\"] = embed[:,0]\n",
    "\tdf[\"y\"] = embed[:,1]\n",
    "\tdf[\"z\"] = embed[:,2]\n",
    "\tsilhouette_samples = []\n",
    "\tsilhouette_score = []\n",
    "\tlinkage_matrices = []\n",
    "\tavg = cluster.AgglomerativeClustering(distance_threshold=None, n_clusters=4, linkage=\"average\")\n",
    "\tavg = avg.fit(dists.to_numpy())\n",
    "\tdflt = \"#000000\"\n",
    "\tD_leaf_colors = {dists.index[i]: colour_list[avg.labels_[i]] for i in range(len(avg.labels_))}\n",
    "\t#Average\n",
    "\tfor link in [\"average\", \"complete\", \"single\", \"ward\"]:\n",
    "\t\tlinkage = cluster.AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage=link)\n",
    "\t\tlinkage = linkage.fit(dists.to_numpy())\n",
    "\t\tcounts = numpy.zeros(linkage.children_.shape[0])\n",
    "\t\tn_samples = len(linkage.labels_)\n",
    "\t\tfor i, merge in enumerate(linkage.children_):\n",
    "\t\t\tcurrent_count = 0\n",
    "\t\t\tfor child_idx in merge:\n",
    "\t\t\t\tif child_idx < n_samples:\n",
    "\t\t\t\t\tcurrent_count += 1  # leaf node\n",
    "\t\t\t\telse:\t\n",
    "\t\t\t\t\tcurrent_count += counts[child_idx - n_samples]\n",
    "\t\t\tcounts[i] = current_count\n",
    "\t\tlinkage_matrix = numpy.column_stack([linkage.children_, linkage.distances_, counts]).astype(float)\n",
    "\t\tlinkage_matrices.append(linkage_matrix)\n",
    "\t\t# notes:\n",
    "\t\t# * rows in Z correspond to \"inverted U\" links that connect clusters\n",
    "\t\t# * rows are ordered by increasing distance\n",
    "\t\t# * if the colors of the connected clusters match, use that color for link\n",
    "\t\tlink_cols = {}\n",
    "\t\tfor i, i12 in enumerate(linkage_matrix[:,:2].astype(int)):\n",
    "\t\t\tc1, c2 = (link_cols[x] if x > len(linkage_matrix) else D_leaf_colors[dists.index[x]] for x in i12)\n",
    "\t\t\tif c1 == c2:\n",
    "\t\t\t\tlink_cols[i+1+len(linkage_matrix)] = c1\n",
    "\t\t\telse:\n",
    "\t\t\t\tif i12[0] < n_samples:\n",
    "\t\t\t\t\tlink_cols[i+1+len(linkage_matrix)] = D_leaf_colors[dists.index[i12[0]]]\n",
    "\t\t\t\telif i12[1] < n_samples:\n",
    "\t\t\t\t\tlink_cols[i+1+len(linkage_matrix)] = D_leaf_colors[dists.index[i12[1]]]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tlink_cols[i+1+len(linkage_matrix)] = dflt\n",
    "\t\t# Dendrogram\n",
    "\t\t# D = dendrogram(Z=linkage_matrix, labels=dists.index, color_threshold=None, no_labels=True, link_color_func=lambda x: link_cols[x])\n",
    "\t\t#plt.xlabel(name+\" \"+link)\n",
    "\t\t# plt.yticks([])\n",
    "\t\t# plt.savefig(dir+\"/\"+name.replace(\" \",\"_\")+\"_\"+link+\"_dendrogram.png\")\n",
    "\t\t# plt.show()\n",
    "\tsse = []\n",
    "\tfor link in [\"average\", \"complete\", \"single\", \"ward\"]:\n",
    "\t\tfor k in cluster_numbers:\n",
    "\t\t\tlinkage = cluster.AgglomerativeClustering(distance_threshold=None, n_clusters=k, linkage=link)\n",
    "\t\t\tlinkage = linkage.fit(dists.to_numpy())\n",
    "\t\t\tlabels = linkage.labels_\n",
    "\t\t\tlabels = [str(c) for c in labels]\n",
    "\t\t\tdf[link+str(k)] = labels\n",
    "\t\t\tsil_score = metrics.silhouette_score(dists, df[link+str(k)], metric=\"precomputed\")\n",
    "\t\t\tsilhouette_score.append(sil_score)\n",
    "\t\t\tsil_samps = metrics.silhouette_samples(dists, df[link+str(k)], metric=\"precomputed\")\n",
    "\t\t\tsilhouette_samples.append([sil_samps])\n",
    "\t\t\t# fig = px.scatter(df, x='x', y='y',color=link+str(k), title=name+\" (\"+link+\" \"+str(k)+\")\", hover_data=[df.index], width=800, height=600, color_discrete_map={\n",
    "            #     \"0\": hex_list[0], \"1\": hex_list[1],\"2\": hex_list[1],\"2\": hex_list[2], \"3\": hex_list[3]})\n",
    "\t\t\t# fig.update_traces(marker={'size': 5})\n",
    "\t\t\t# if show2d:\n",
    "\t\t\t# \tfig.show()\n",
    "\t\t\t# fig.write_image(dir+\"/\"+name.replace(\" \",\"_\")+\"_\"+link+str(k)+\"_2D.png\")\n",
    "\t# if showElbow:\n",
    "\t# \tplt.plot(cluster_numbers, sse)\n",
    "\t# \tplt.title(\"Elbow Method\")\n",
    "\t# \tplt.xlabel(\"Number of Clusters\")\n",
    "\t# \tplt.xticks(cluster_numbers)\n",
    "\t# \tplt.ylabel(\"SSE\")\n",
    "\t# \tplt.savefig(dir+\"/\"+name.replace(\" \",\"_\")+\"_kmeans-elbow.png\")\n",
    "\t# \tfig.show()\n",
    "\tdf.to_csv(dir+\"/\"+name.replace(\" \",\"_\")+\"_df.csv\")\n",
    "\trand_ind_tables_latex = []\n",
    "\tpercentages_dict = {}\n",
    "\tfor k in cluster_numbers:\n",
    "\t\trand_ind_tables_latex.append(generate_rand_index(df, [\"average\"+str(k), \"complete\"+str(k), \"single\"+str(k),\"ward\"+str(k)]).to_latex())\n",
    "\t\tprint(\"cluster sizes and percentages are:\")\n",
    "\t\tpercentages_dict[\"average{}\".format(k)], counts = population_percentages(df, \"average\"+str(k))\n",
    "\t\tprint(\"average{}\".format(k), counts, percentages_dict[\"average{}\".format(k)])\n",
    "\t\tpercentages_dict[\"complete{}\".format(k)], counts = population_percentages(df, \"complete\"+str(k))\n",
    "\t\tprint(\"complete{}\".format(k), counts, percentages_dict[\"complete{}\".format(k)])\n",
    "\t\tpercentages_dict[\"single{}\".format(k)], counts = population_percentages(df, \"single\"+str(k))\n",
    "\t\tprint(\"single{}\".format(k), counts, percentages_dict[\"single{}\".format(k)])\n",
    "\t\tpercentages_dict[\"ward{}\".format(k)], counts = population_percentages(df, \"ward\"+str(k))\n",
    "\t\tprint(\"ward{}\".format(k), counts, percentages_dict[\"ward{}\".format(k)])\n",
    "\t\t#percentages_dict[\"kmeans++{}\".format(k)] = population_percentages(df, \"kmeans++\"+str(k))\n",
    "\t\t#print(\"kmeans++{}\".format(k), percentages_dict[\"kmeans++{}\".format(k)])\n",
    "\treturn df, silhouette_score, silhouette_samples, rand_ind_tables_latex, percentages_dict, dists, linkage_matrices\n",
    "\n",
    "def get_main_population(analysis, cluster : str, dists : pandas.DataFrame):\n",
    "\tmain_id = analysis[4][cluster].index(max(analysis[4][cluster]))\n",
    "\tmain_index= analysis[0].index[analysis[0][cluster] == str(main_id)]\n",
    "\treturn dists[main_index].loc[main_index]\n",
    "\n",
    "def purity(merges, clus : set, n_objs):\n",
    "\tdct = dict([(i, {i}) for i in range(n_objs)])\n",
    "\tfor i, row in enumerate(merges, n_objs):\n",
    "\t\tdct[i] = dct[row[0]].union(dct[row[1]])\n",
    "\t\tdel dct[row[0]]\n",
    "\t\tdel dct[row[1]]\n",
    "\t\tfor c in list(dct.values()):\n",
    "\t\t\tif clus.issubset(c):\n",
    "\t\t\t\treturn c, (n_objs-len(c))/(n_objs-len(clus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X1 Dataset Analysis\n",
    "\n",
    "The X1 dataset contains 140 hMSC (human mesenchymal stem cells) cells. We will:\n",
    "1. Load the cell contours and nucleus centers\n",
    "2. Compute persistence diagrams for each cell\n",
    "3. Calculate Wasserstein distances between all pairs\n",
    "4. Perform hierarchical clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X1 Dataset Setup\n",
    "\n",
    "# Set dataset parameters for X1\n",
    "dataset_X1 = \"X1\"\n",
    "width_X1 = 1500  # Largest width of an image in the X1 dataset (FilamentSensor2 pads all images to same size)\n",
    "height_X1 = 1692  # Largest height of an image in the X1 dataset\n",
    "conversion_factor_X1 = 0.3155\n",
    "# Get all cell files\n",
    "files_X1 = os.listdir(\"../\" + dataset_X1 + \"/cell/raw_images\")\n",
    "cell_names_X1 = sorted([int(file[0:3]) for file in files_X1 if file.endswith('.tif')])\n",
    "n_cells_X1 = len(cell_names_X1)\n",
    "print(f\"X1 dataset contains {n_cells_X1} cells\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1: Load Nucleus Centers\n",
    "\n",
    "First, we will load the file containing the center of the nucleus for each cell.\n",
    "\n",
    "Next, for each cell we will calculate the persistence diagram using a radial function based at the center of the nucleus. We need to translate the center of the nucleus into the same frame as the contour is in. This is due to the way FilamentSensor extracts the contour of each cell in a directory. The appropriate `height` and `width` values can be found in the [FilamentSensor2](https://filament-sensor.de/) log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_X1 = pandas.read_csv(\"../\" + dataset_X1 + \"/Nuc_Cm_\" + dataset_X1 + \".csv\", index_col=\"filename\")\n",
    "print(f\"Loaded nucleus centers for {len(centers_X1)} cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1: Compute Persistence Diagrams\n",
    "\n",
    "Now we compute the persistence diagram for each cell using the radial function based at the nucleus center.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours_X1 = []\n",
    "for i in cell_names_X1:\n",
    "\tactin = tifffile.imread(\"../\" + dataset_X1 + \"/cell/raw_images/\" + str(\"%03d\" % i) + \".tif\")\n",
    "\theight_diff = height_X1 - actin.shape[0]\n",
    "\twidth_diff = width_X1 - actin.shape[1]\n",
    "\t\n",
    "\t# Create polygon with focal point at nucleus center\n",
    "\tc_i = correa.create_polygon_focal_point(\n",
    "\t\t\"../\" + dataset_X1 + \"/cell/contours/\" + str(\"%03d\" % i) + \"_contour.csv\", \n",
    "\t\t[centers_X1.loc[i, \"X_m\"] + width_diff/2, centers_X1.loc[i, \"Y_m\"] + height_diff/2], convert_to_microns_factor=conversion_factor_X1\n",
    "\t)\n",
    "\tc_i.persistence_diagram()\n",
    "\tcontours_X1.append(c_i)\n",
    "\t\n",
    "\tif (i+1) % 20 == 0:\n",
    "\t\tprint(f\"Processed {i+1}/{n_cells_X1} cells\")\n",
    "\n",
    "print(f\"Completed persistence diagrams for all {n_cells_X1} cells\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1: Visualize Sample Contours\n",
    "\n",
    "We can visualize some sample contours and mark the center of the nucleus (shown in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first 5 cells\n",
    "for i in cell_names_X1[0:5]:\n",
    "\tcontour = pandas.read_csv(dataset_X1 + \"/cell/contours/\" + str(\"%03d\" % i) + \"_contour.csv\", header=None)\n",
    "\tactin = tifffile.imread(dataset_X1 + \"/cell/raw_images/\" + str(\"%03d\" % i) + \".tif\")\n",
    "\theight_diff = height_X1 - actin.shape[0]\n",
    "\twidth_diff = width_X1 - actin.shape[1]\n",
    "\tcenter = pandas.DataFrame(\n",
    "\t\t[[centers_X1.loc[i, \"X_m\"] + width_diff/2, centers_X1.loc[i, \"Y_m\"] + height_diff/2]], \n",
    "\t\tcolumns=[\"x\", \"y\"]\n",
    "\t)\n",
    "\t\n",
    "\tfig_data = px.scatter(contour, x=0, y=1, width=800, height=600).data\n",
    "\tfig_data = fig_data + px.scatter(center, x=\"x\", y=\"y\").update_traces(marker={'size': 10, 'color': 'Red'}).data\n",
    "\tfig = go.Figure(fig_data)\n",
    "\tfig.update_layout(title=f\"X1 Cell {i:03d}\", xaxis_title=\"X\", yaxis_title=\"Y\")\n",
    "\tfig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1: Compute Wasserstein Distances\n",
    "\n",
    "Once we have a persistence diagram for each cell summarising its morphology, we compute the Wasserstein distance between each pair of persistence diagrams as a (dis)similarity score for each pair of cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_distances_X1 = numpy.zeros((n_cells_X1, n_cells_X1))\n",
    "for i in range(n_cells_X1):\n",
    "\tfor j in range(i, n_cells_X1):\n",
    "\t\tdist_ij = correa.wasserstein_distance(contours_X1[i], contours_X1[j], q=2)\n",
    "\t\tdist_ji = correa.wasserstein_distance(contours_X1[j], contours_X1[i], q=2)\n",
    "\t\tdist = (dist_ij + dist_ji) / 2\n",
    "\t\tw_distances_X1[i,j] = dist\n",
    "\t\tw_distances_X1[j,i] = dist\n",
    "\t\n",
    "\tif (i+1) % 10 == 0:\n",
    "\t\tprint(f\"Computed distances for {i+1}/{n_cells_X1} cells\")\n",
    "\n",
    "print(f\"Completed Wasserstein distance computation for all {n_cells_X1} cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1: Distance Heatmap\n",
    "\n",
    "Next we display a heatmap of the Wasserstein distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(w_distances_X1, width=800, height=800)\n",
    "fig.update_layout(title=\"X1 Wasserstein Distance Heatmap\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_X1 = pandas.DataFrame(w_distances_X1, columns=cell_names_X1, index=cell_names_X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1: Hierarchical Clustering Analysis\n",
    "\n",
    "Perform hierarchical clustering with different linkage methods (average, complete, single, ward) and different numbers of clusters (3, 4, 5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_X1 = analysis(dists_X1, [3,4,5], \"X1\", \"X1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1: Compute purity scores based on ```average4```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(4):\n",
    "\tclus = set(())\n",
    "\tfor i in range(len(A_X1[0][\"average4\"])):\n",
    "\t\tif A_X1[0][\"average4\"].iloc[i] == str(k):\n",
    "\t\t\tclus.add(i)\n",
    "\tprint(f\"\\nCluster {k}:\")\n",
    "\tprint(f\"  Average linkage purity: {purity(A_X1[6][0], clus, n_c)[1]:.4f}\")\n",
    "\tprint(f\"  Complete linkage purity: {purity(A_X1[6][1], clus, n_c)[1]:.4f}\")\n",
    "\tprint(f\"  Single linkage purity: {purity(A_X1[6][2], clus, n_c)[1]:.4f}\")\n",
    "\tprint(f\"  Ward linkage purity: {purity(A_X1[6][3], clus, n_c)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1: Identify Outliers\n",
    "\n",
    "In the X1 dataset, the heatmap and dendrograms indicate there is an outlier. Let's identify which cell this is. Using `average4`, the outlier has cluster number 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in A_X1[0].index:\n",
    "\tif int(A_X1[0].loc[i][\"average4\"]) == 3:\n",
    "\t\tprint(f\"Outlier cell: {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1: Analysis Excluding Outlier\n",
    "\n",
    "If desired, we can use the `analysis` command with the `exclude` parameter to exclude the outlier cell (015) from our analysis and compute purity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_X1_main = analysis(dists_X1, [3,4,5], \"X1\", \"X1\", exclude=[\"X1-15\"])\n",
    "\n",
    "n_c = A_X1_main[0].shape[0]\n",
    "print(f\"Number of cells (excluding outlier): {n_c}\")\n",
    "\n",
    "for k in range(4):\n",
    "\tclus = set(())\n",
    "\tfor i in range(len(A_X1_main[0][\"average4\"])):\n",
    "\t\tif A_X1_main[0][\"average4\"].iloc[i] == str(k):\n",
    "\t\t\tclus.add(i)\n",
    "\tprint(f\"\\nCluster {k}:\")\n",
    "\tprint(f\"  Average linkage purity: {purity(A_X1_main[6][0], clus, n_c)[1]:.4f}\")\n",
    "\tprint(f\"  Complete linkage purity: {purity(A_X1_main[6][1], clus, n_c)[1]:.4f}\")\n",
    "\tprint(f\"  Single linkage purity: {purity(A_X1_main[6][2], clus, n_c)[1]:.4f}\")\n",
    "\tprint(f\"  Ward linkage purity: {purity(A_X1_main[6][3], clus, n_c)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y1 Dataset Analysis\n",
    "\n",
    "The Y1 dataset contains 100 HeLa cells. We will follow the same analysis pipeline as for X1:\n",
    "1. Load the cell contours and nucleus centers\n",
    "2. Compute persistence diagrams for each cell\n",
    "3. Calculate Wasserstein distances between all pairs\n",
    "4. Perform hierarchical clustering analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y1 Dataset Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset parameters for Y1\n",
    "dataset_Y1 = \"Y1\"\n",
    "width_Y1 = 1226  # Largest width of an image in the Y1 dataset\n",
    "height_Y1 = 1088  # Largest height of an image in the Y1 dataset\n",
    "conversion_factor_Y1 = 0.1639\n",
    "# Get all cell files\n",
    "files_Y1 = os.listdir(\"../\" + dataset_Y1 + \"/cell/raw_images\")\n",
    "cell_names_Y1 = sorted([int(file[0:3]) for file in files_Y1 if file.endswith('.tif')])\n",
    "n_cells_Y1 = len(cell_names_Y1)\n",
    "print(f\"Y1 dataset contains {n_cells_Y1} cells\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y1: Load Nucleus Centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_Y1 = pandas.read_csv(\"../\" + dataset_Y1 + \"/Nuc_Cm_\" + dataset_Y1 + \".csv\", index_col=\"filename\")\n",
    "print(f\"Loaded nucleus centers for {len(centers_Y1)} cells\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y1: Compute Persistence Diagrams\n",
    "\n",
    "Compute the persistence diagram for each HeLa cell using the radial function based at the nucleus center.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours_Y1 = []\n",
    "for i in cell_names_Y1:\n",
    "\tactin = tifffile.imread(dataset_Y1 + \"/cell/raw_images/\" + str(\"%03d\" % i) + \".tif\")\n",
    "\theight_diff = height_Y1 - actin.shape[0]\n",
    "\twidth_diff = width_Y1 - actin.shape[1]\n",
    "\t\n",
    "\t# Create polygon with focal point at nucleus center\n",
    "\tc_i = correa.create_polygon_focal_point(\n",
    "\t\t\"../\" + dataset_Y1 + \"/cell/contours/\" + str(\"%03d\" % i) + \"_contour.csv\", \n",
    "\t\t[centers_Y1.loc[i, \"X_m\"] + width_diff/2, centers_Y1.loc[i, \"Y_m\"] + height_diff/2], convert_to_microns_factor=conversion_factor_Y1\n",
    "\t)\n",
    "\tc_i.persistence_diagram()\n",
    "\tcontours_Y1.append(c_i)\n",
    "\t\n",
    "\tif (i+1) % 20 == 0:\n",
    "\t\tprint(f\"Processed {i+1}/{n_cells_Y1} cells\")\n",
    "\n",
    "print(f\"Completed persistence diagrams for all {n_cells_Y1} cells\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y1: Compute Wasserstein Distances\n",
    "\n",
    "Compute the Wasserstein distance between each pair of HeLa cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_distances_Y1 = numpy.zeros((n_cells_Y1, n_cells_Y1))\n",
    "for i in range(n_cells_Y1):\n",
    "\tfor j in range(i, n_cells_Y1):\n",
    "\t\tdist_ij = correa.wasserstein_distance(contours_Y1[i], contours_Y1[j], q=2)\n",
    "\t\tdist_ji = correa.wasserstein_distance(contours_Y1[j], contours_Y1[i], q=2)\n",
    "\t\tdist = (dist_ij + dist_ji) / 2\n",
    "\t\tw_distances_Y1[i,j] = dist\n",
    "\t\tw_distances_Y1[j,i] = dist\n",
    "\t\n",
    "\tif (i+1) % 10 == 0:\n",
    "\t\tprint(f\"Computed distances for {i+1}/{n_cells_Y1} cells\")\n",
    "\n",
    "print(f\"Completed Wasserstein distance computation for all {n_cells_Y1} cells\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y1: Distance Heatmap\n",
    "\n",
    "Display a heatmap of the Wasserstein distances for the Y1 dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(w_distances_Y1, width=800, height=800)\n",
    "fig.update_layout(title=\"Y1 Wasserstein Distance Heatmap\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_Y1 = pandas.DataFrame(w_distances_Y1, columns=cell_names_Y1, index=cell_names_Y1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y1: Hierarchical Clustering Analysis\n",
    "\n",
    "Perform hierarchical clustering for the Y1 dataset with different linkage methods and numbers of clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_Y1 = analysis(dists_Y1, [3,4,5], \"Y1\", \"Y1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y1: Compute purity scores based on ```average4```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(4):\n",
    "\tclus = set(())\n",
    "\tfor i in range(len(A_Y1[0][\"average4\"])):\n",
    "\t\tif A_Y1[0][\"average4\"].iloc[i] == str(k):\n",
    "\t\t\tclus.add(i)\n",
    "\tprint(f\"\\nCluster {k}:\")\n",
    "\tprint(f\"  Average linkage purity: {purity(A_Y1[6][0], clus, n_c)[1]:.4f}\")\n",
    "\tprint(f\"  Complete linkage purity: {purity(A_Y1[6][1], clus, n_c)[1]:.4f}\")\n",
    "\tprint(f\"  Single linkage purity: {purity(A_Y1[6][2], clus, n_c)[1]:.4f}\")\n",
    "\tprint(f\"  Ward linkage purity: {purity(A_Y1[6][3], clus, n_c)[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X1Y1 Combined Dataset Analysis\n",
    "\n",
    "The X1Y1 combined dataset contains 240 cells (140 hMSC from X1 and 100 HeLa from Y1). This analysis will:\n",
    "1. Combine the contours and persistence diagrams from both datasets\n",
    "2. Compute Wasserstein distances between all pairs of cells across both types\n",
    "3. Perform hierarchical clustering to see if the two cell types separate\n",
    "4. Compare results with ground truth labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1Y1: Combine Datasets\n",
    "\n",
    "Combine the contours from X1 and Y1 datasets and create a combined cell name list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine contours from both datasets\n",
    "contours_X1Y1 = contours_X1 + contours_Y1\n",
    "\n",
    "# Create combined cell names with dataset prefix\n",
    "cell_names_X1Y1 = [f\"X1_{i:03d}\" for i in cell_names_X1] + [f\"Y1_{i:03d}\" for i in cell_names_Y1]\n",
    "n_cells_X1Y1 = len(cell_names_X1Y1)\n",
    "\n",
    "print(f\"Combined X1Y1 dataset contains {n_cells_X1Y1} cells\")\n",
    "print(f\"  - X1 (hMSC): {n_cells_X1} cells\")\n",
    "print(f\"  - Y1 (HeLa): {n_cells_Y1} cells\")\n",
    "\n",
    "# Create ground truth labels (1 for X1, 2 for Y1)\n",
    "ground_truth_2clusters = [1] * n_cells_X1 + [2] * n_cells_Y1\n",
    "print(f\"\\nGround truth: {n_cells_X1} cells in cluster 1 (X1), {n_cells_Y1} cells in cluster 2 (Y1)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1Y1: Compute Combined Distance Matrix\n",
    "\n",
    "Compute the Wasserstein distance matrix for the combined dataset. We can reuse the within-dataset distances and only compute the cross-dataset distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the combined distance matrix\n",
    "w_distances_X1Y1 = numpy.zeros((n_cells_X1Y1, n_cells_X1Y1))\n",
    "\n",
    "# Copy X1 within-dataset distances\n",
    "w_distances_X1Y1[:n_cells_X1, :n_cells_X1] = w_distances_X1\n",
    "print(\"Copied X1 within-dataset distances\")\n",
    "\n",
    "# Copy Y1 within-dataset distances\n",
    "w_distances_X1Y1[n_cells_X1:, n_cells_X1:] = w_distances_Y1\n",
    "print(\"Copied Y1 within-dataset distances\")\n",
    "\n",
    "# Compute cross-dataset distances (X1 vs Y1)\n",
    "print(\"Computing cross-dataset distances (X1 vs Y1)...\")\n",
    "for i in range(n_cells_X1):\n",
    "\tfor j in range(n_cells_Y1):\n",
    "\t\tdist_ij = correa.wasserstein_distance(contours_X1[i], contours_Y1[j], q=2)\n",
    "\t\tdist_ji = correa.wasserstein_distance(contours_Y1[j], contours_X1[i], q=2)\n",
    "\t\tdist = (dist_ij + dist_ji) / 2\n",
    "\t\tw_distances_X1Y1[i, n_cells_X1 + j] = dist\n",
    "\t\tw_distances_X1Y1[n_cells_X1 + j, i] = dist\n",
    "\t\n",
    "\tif (i+1) % 20 == 0:\n",
    "\t\tprint(f\"  Computed cross-distances for X1 cell {i+1}/{n_cells_X1}\")\n",
    "\n",
    "print(f\"Completed combined distance matrix for all {n_cells_X1Y1} cells\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1Y1: Distance Heatmap\n",
    "\n",
    "Display a heatmap of the combined distance matrix. The block structure should show lower distances within each cell type and higher distances between cell types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(w_distances_X1Y1, width=900, height=900)\n",
    "fig.update_layout(\n",
    "\ttitle=\"X1Y1 Combined Wasserstein Distance Heatmap\",\n",
    "\txaxis_title=\"Cell Index\",\n",
    "\tyaxis_title=\"Cell Index\"\n",
    ")\n",
    "# Add annotations to show dataset boundaries\n",
    "fig.add_vline(x=n_cells_X1-0.5, line_width=2, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.add_hline(y=n_cells_X1-0.5, line_width=2, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_X1Y1 = pandas.DataFrame(w_distances_X1Y1, columns=cell_names_X1Y1, index=cell_names_X1Y1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1Y1: Hierarchical Clustering Analysis\n",
    "\n",
    "Perform hierarchical clustering on the combined dataset to see if the two cell types separate naturally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_X1Y1 = analysis(dists_X1Y1, [2,3,4,5], \"X1Y1\", \"X1Y1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X1Y1: Evaluate Clustering Quality\n",
    "\n",
    "Peform the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_X1Y1 = analysis(dists_X1Y1, [2,3,4,5,6,7,8,9,10], \"X1Y1\", \"X1Y1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
